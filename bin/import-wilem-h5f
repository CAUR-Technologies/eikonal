#!/usr/bin/env python
import agstd.log
import os, sys, re, time, datetime, glob
import dateutil.parser

import numpy as np

import scipy as sc
import scipy.optimize

import agstd.sdb.dbase as sdb

import logging
log = logging.root
log.setLevel(logging.DEBUG)


description_re = re.compile('[A-Za-z_0-9]+[\s]*=[^=^\n]*\s')
filename_re = re.compile('(?P<year>[0-9][0-9][0-9][0-9])'
                         '(?P<month>[\w][\w][\w])'
                         '(?P<day>[0-9][0-9])'
                         '(?P<hour>[0-9][0-9])'
                         '(?P<min>[0-9][0-9])'
                         '(?P<sec>[0-9][0-9])'
                         '(?P<ms>[0-9][0-9][0-9])')

p_event_re = re.compile('P-wave .*')
s_event_re = re.compile('S-wave .*')

time_re = re.compile('([0-9][0-9]):([0-9][0-9]):([0-9][0-9])\.([0-9][0-9][0-9][0-9][0-9][0-9])')
etime_re = re.compile('([0-9][0-9]):([0-9][0-9]):([0-9][0-9])\.([0-9][0-9][0-9])')

def parse_description(filename, dirname, **kw):
    s = open(os.path.join(dirname, filename)).read()
    additional_kw = [t for t in kw.iteritems()] + [('filename', filename)]
    s_s = [["S-%s" % k for k in description_re.findall(ps)] for ps in s_event_re.findall(s)]
    s_p = [["P-%s" % k for k in description_re.findall(ps)] for ps in p_event_re.findall(s)]
    if len(s_s) != 0:
        additional_kw.extend([[k.strip() for k in s_s1.strip().split('=')] for s_s1 in s_s[0]])
    if len(s_p) != 0:
        additional_kw.extend([[k.strip() for k in s_s1.strip().split('=')] for s_s1 in s_p[0]])
    return dict([[k.strip() for k in t.strip().split('=')] for t in description_re.findall(s)] + additional_kw)

def parse_dat(filename, dirname):
    fid = open(os.path.join(dirname, filename))
    tmp = fid.readline()
    nb_components = int(tmp.split()[0])
    sampling_rate = float(tmp.split()[1])

    tmp = fid.readline()

    seisX = np.zeros(1)
    seisY = np.zeros(1)
    seisZ = np.zeros(1)

    if nb_components == 3:
        nb_points = int(tmp)
        seisX = np.array([float(fid.readline()) for i in range(0,nb_points)])
        
        nb_points = int(tmp)
        seisY = np.array([float(fid.readline()) for i in range(0,nb_points)])
        
        nb_points = int(tmp)
        seisZ = np.array([float(fid.readline()) for i in range(0,nb_points)])

    else :
        nb_points = int(tmp)
        seisX = np.array([float(fid.readline()) for i in range(0,nb_points)])
    
    return nb_components, sampling_rate, seisX, seisY, seisZ

def toEventTuple(description):
    tlist = [int(t) for t in etime_re.match(description['TIME']).groups()]
    t = tlist[0] * 60 * 60 + tlist[1] * 60 + tlist[2] + tlist[3] / 1.0e6
    d = int(time.mktime(dateutil.parser.parse(description['DATE']).timetuple()))
    return (d, description['filename'],
            float(description['P-Mo']), float(description['P-E']), float(description['P-f']),
            float(description['S-Mo']), float(description['S-E']), float(description['S-f']),
            float(description['X']), float(description['Y']), float(description['Z']), t)

def toTrgTuple(description, station_id):
    return (station_id,
            description['event_id'],
            float(description['sample_rate']),
            float(description['time']), int(description['Pos']),
            float(description['Q0']), float(description['Q']),
            float(description['NATURAL_FREQUENCY']), float(description['DAMPING_FACTOR']),
            description['METER_TYPE'],
            description['filename'], float(description['HYPO_DISTANCE']))

def toPickSTuple(description, trigger_id):
    time = float(description['time']) + (int(description['S']) - int(description['Pos'])) / float(description['sample_rate'])
    return (trigger_id, float(description['Vs']), time, int(description['S']), sdb.pick_type.S_Wave)

def toPickPTuple(description, trigger_id):
    time = float(description['time']) + (int(description['P']) - int(description['Pos'])) / float(description['sample_rate'])
    return (trigger_id, float(description['Vp']), time, int(description['P']), sdb.pick_type.P_Wave)

def toStationTuple(description, name = ""):
    return (float(description['X']), float(description['Y']), float(description['Z']), "")


class Event(object):
    def __init__(self, date, origin, P_Mo, S_Mo, P_E, S_E, P_f, S_f, db = None):
        self.origin = origin


    @classmethod
    def from_file(cls, basename, dirname):
        description = parse_description(basename, dirname)


    def description(self):
        pass


class Station(object):
    def __init__(self, X, Y, Z):
        pass






if __name__ == '__main__':
    dbfile = sys.argv[1]
    group = sys.argv[2]
    db = sdb.SeismicHDF5DB(dbfile, group, mode = 'a')
    for indir in sys.argv[3:]:
        start_time = time.time()
        filenames = os.listdir(indir)
        filenames.sort()

        log.info("%s --> %d files" % (indir, len(filenames)))

        # ADDING TABLES
        log.info("     --> Processing Events ...")

        robust_splitary = []
        for evnlst in np.split(filenames, [i+1 for i, f in enumerate(filenames[1:]) if f.endswith('evn')]):
            trgs = [(t[:-4] + ".dat", t) for t in evnlst[1:] if t.endswith('trg')]
            robust_splitary.append((evnlst[0], trgs))
        evnids = range(db.root.events.nrows, db.root.events.nrows + len(robust_splitary))
        log.info("              --> Processing Description ...")
        evndesc = [parse_description(ary[0], indir) for ary in robust_splitary]
        print evndesc
        db.root.events.append([toEventTuple(evn) for evn in evndesc])
        db.root.events.flush()

        log.info("     --> Processing Triggers ...")
        trgdesc = []
        n = 0
        log.info("              --> Processing Description ...")
        for ary, evtid in zip(robust_splitary, evnids):
            trgdesc.extend([(parse_description(trg, indir, event_id = evtid), dat) for dat, trg in ary[1]])

        log.info("              --> Processing Time ...")
        for trg, dat in trgdesc:
            tlist = [int(t) for t in time_re.match(trg['TT']).groups()]
            trg['time'] = tlist[0] * 60 * 60 + tlist[1] * 60 + tlist[2] + tlist[3] / 1.0e6

        station_id = [db.add_station(toStationTuple(trg)) for trg, dat in trgdesc]

        trgids = range(db.root.triggers.description.nrows, db.root.triggers.description.nrows + len(trgdesc))
        db.root.triggers.description.append([toTrgTuple(trg, sid) for (trg, dat), sid in zip(trgdesc, station_id)])
        db.root.triggers.description.flush()

        pickS = [toPickSTuple(trg, tid) for (trg, dat), tid in zip(trgdesc, trgids) if float(trg['S']) != 0]
        pickP = [toPickPTuple(trg, tid) for (trg, dat), tid in zip(trgdesc, trgids) if float(trg['P']) != 0]

        if len(pickS) == 0:
            pick = np.rec.array(pickP)
        elif len(pickP) == 0:
            pick = np.rec.array(pickS)
        else:
            pick = np.concatenate([np.rec.array(pickP), np.rec.array(pickS)])
        pick.sort(order = 'f0')
        db.root.triggers.pick.append(pick.tolist())
        db.root.triggers.pick.flush()

        log.info("     --> Processing Seismograms ...")
        for (trg, dat), tid in zip(trgdesc, trgids):
            row = db.root.seismograms.row
            try:
                nb_components, sampling_rate, seisX,seisY, seisZ = parse_dat(dat,indir)
                print nb_components,sampling_rate,seisX
                lenX,lenY,lenZ = len(seisX),len(seisY),len(seisZ)
                seisX.resize(20000)
                seisY.resize(20000)
                seisZ.resize(20000)
                row['seisX'] = seisX
                row['seisY'] = seisY
                row['seisZ'] = seisZ
                row['station_id'] = int(sid)
                row['event_id'] = int(evtid)
                row['sampling_rate'] = sampling_rate
                row['length'] = [lenX,lenY,lenZ]
            except Exception,e:
                log.error("Error seismogram file : %s zeroing ... " % dat)
                row['seisX'] = np.zeros(20000)
                row['seisY'] = np.zeros(20000)
                row['seisZ'] = np.zeros(20000)
                row['station_id'] = int(sid)
                row['event_id'] = int(evtid)
                row['sampling_rate'] = sampling_rate
                row['length'] = [0,0,0]
                
            row.append()
            db.root.seismograms.flush()
        log.info("Process Completed in <%f seconds>" % (time.time() - start_time))


